{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Retina Damage From Optical Coherence Tomography (OCT) Images, using Transfer Learning on VGG16 CNN Model\n",
    "## Context\n",
    "Retinal Optical Coherence Tomography (OCT) is an imaging technique used to capture high-resolution cross sections of the retinas of living patients. Approximately 30 million OCT scans are performed each year, and the analysis and interpretation of these images takes up a significant amount of time (Swanson and Fujimoto, 2017).\n",
    "\n",
    "![Figure 1.](https://i.imgur.com/fSTeZMd.png)\n",
    "\n",
    "Figure 1. Representative Optical Coherence Tomography Images and the Workflow Diagram \\[Kermany et. al. 2018\\]\n",
    "\n",
    "(A) (Far left) Choroidal Neo-Vascularization (CNV) with neovascular membrane (white arrowheads) and associated subretinal fluid (arrows). (Middle left) Diabetic Macular Edema (DME) with retinal-thickening-associated intraretinal fluid (arrows). (Middle right) Multiple drusen (arrowheads) present in early AMD. (Far right) Normal retina with preserved foveal contour and absence of any retinal fluid/edema.\n",
    "\n",
    "## Content\n",
    "* The dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (NORMAL,CNV,DME,DRUSEN). There are 84,495 X-Ray images (JPEG) and 4 categories (NORMAL,CNV,DME,DRUSEN).\n",
    "* Images are labeled as (disease)-(randomized patient ID)-(image number by this patient) and split into 4 directories: CNV, DME, DRUSEN, and NORMAL.\n",
    "\n",
    "* Optical coherence tomography (OCT) images (Spectralis OCT, Heidelberg Engineering, Germany) were selected from retrospective cohorts of adult patients from the Shiley Eye Institute of the University of California San Diego, the California Retinal Research Foundation, Medical Center Ophthalmology Associates, the Shanghai First Peopleâ€™s Hospital, and Beijing Tongren Eye Center between July 1, 2013 and March 1, 2017.\n",
    "\n",
    "## Acknowledgements\n",
    "* Data: https://data.mendeley.com/datasets/rscbjbr9sj/2\n",
    "* Citation: http://www.cell.com/cell/fulltext/S0092-8674(18)30154-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install keract\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import expand_dims\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sn\n",
    "from skimage.transform import resize\n",
    "from skimage.color import gray2rgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from IPython.display import SVG\n",
    "import keract\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import applications, optimizers\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.utils import to_categorical, model_to_dot, plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataset and Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_dir = \"C:/Users/Admin/Downloads/ML_lab/Assignment_2_ML/\"\n",
    "train_data_dir = \"C:/Users/Admin/Downloads/ML_lab/Assignment_2_ML/train\"\n",
    "val_data_dir = \"C:/Users/Admin/Downloads/ML_lab/Assignment_2_ML/validation\"\n",
    "test_data_dir = \"C:/Users/Admin/Downloads/ML_lab/Assignment_2_ML/test\"\n",
    "img_width, img_height = 150, 150 \n",
    "channels = 3\n",
    "batch_size = 32\n",
    "train_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnv_images = len(glob(train_data_dir + '/CNV/*.jpeg'))\n",
    "dme_images = len(glob(train_data_dir + '/DME/*.jpeg'))\n",
    "drusen_images = len(glob(train_data_dir + '/DRUSEN/*.jpeg'))\n",
    "normal_images = len(glob(train_data_dir + '/NORMAL/*.jpeg'))\n",
    "data= {'CNV': cnv_images, 'DME': dme_images, 'DRUSEN': drusen_images, 'NORMAL': normal_images}\n",
    "labels = list(data.keys()) \n",
    "count = list(data.values()) \n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)\n",
    "plt.bar(labels, count, color=['tab:red', 'tab:green', 'tab:blue', 'tab:orange'])\n",
    "plt.axis('on')\n",
    "plt.xlabel(\"Labels\") \n",
    "plt.ylabel(\"Count\") \n",
    "plt.savefig('labels_vs_counts.png', transparent= False, bbox_inches= 'tight', dpi= 400)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Histogram (Tonal Distribution)\n",
    "* Histogram of a normal retina image in the train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image = mpimg.imread(data_dir + 'train/NORMAL/NORMAL-9863816-1.jpeg')\n",
    "color_img= gray2rgb(resize(image, (128, 128)))\n",
    "plt.rcParams['figure.figsize'] = (8.0, 5.0)\n",
    "_ = plt.hist(color_img.ravel(), bins = 256, color = 'orange', )\n",
    "_ = plt.hist(color_img[:, :, 0].ravel(), bins = 256, color = 'red', alpha = 0.5)\n",
    "_ = plt.hist(color_img[:, :, 1].ravel(), bins = 256, color = 'Green', alpha = 0.5)\n",
    "_ = plt.hist(color_img[:, :, 2].ravel(), bins = 256, color = 'Blue', alpha = 0.5)\n",
    "_ = plt.xlabel('Intensity Value')\n",
    "_ = plt.ylabel('Count')\n",
    "_ = plt.legend(['Total', 'Red_Channel', 'Green_Channel', 'Blue_Channel'])\n",
    "plt.savefig('histogram.png', transparent= False, bbox_inches= 'tight', dpi= 400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Images from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Normal\")\n",
    "multipleImages = glob(data_dir + 'train/NORMAL/**')\n",
    "i = 0\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "for im in multipleImages[:25]:\n",
    "    image = mpimg.imread(im)\n",
    "    image_resized = resize(image, (128, 128)) \n",
    "    plt.subplot(5, 5, i+1) #.set_title(l)\n",
    "    plt.imshow(gray2rgb(image_resized)); plt.axis('off')\n",
    "    i += 1\n",
    "plt.savefig('normal_eye.png', transparent= False, bbox_inches= 'tight', dpi= 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Choroidal Neo-Vascularization (CNV)\")\n",
    "multipleImages = glob(data_dir + 'train/CNV/**')\n",
    "i = 0\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "for im in multipleImages[:25]:\n",
    "    image = mpimg.imread(im)\n",
    "    image_resized = resize(image, (128, 128)) \n",
    "    plt.subplot(5, 5, i+1) #.set_title(l)\n",
    "    plt.imshow(gray2rgb(image_resized)); plt.axis('off')\n",
    "    i += 1\n",
    "plt.savefig('cnv_eye.png', transparent= False, bbox_inches= 'tight', dpi= 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Diabetic Macular Edema (DME)\") \n",
    "multipleImages = glob(data_dir + 'train/DME/**')\n",
    "i = 0\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "for im in multipleImages[:25]:\n",
    "    image = mpimg.imread(im)\n",
    "    image_resized = resize(image, (128, 128)) \n",
    "    plt.subplot(5, 5, i+1) #.set_title(l)\n",
    "    plt.imshow(gray2rgb(image_resized)); plt.axis('off')\n",
    "    i += 1\n",
    "plt.savefig('dme_eye.png', transparent= False, bbox_inches= 'tight', dpi= 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Drusen\")\n",
    "multipleImages = glob(data_dir + 'train/DRUSEN/**')\n",
    "i = 0\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "for im in multipleImages[:25]:\n",
    "    image = mpimg.imread(im)\n",
    "    image_resized = resize(image, (128, 128)) \n",
    "    plt.subplot(5, 5, i+1) #.set_title(l)\n",
    "    plt.imshow(gray2rgb(image_resized)); plt.axis('off')\n",
    "    i += 1\n",
    "plt.savefig('drusen_eye.png', transparent= False, bbox_inches= 'tight', dpi= 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale= 1./255,\n",
    "    zoom_range= (0.73, 0.9),\n",
    "    horizontal_flip= True,\n",
    "    rotation_range= 10,\n",
    "    width_shift_range= 0.10,\n",
    "    fill_mode= 'constant',\n",
    "    height_shift_range= 0.10,   \n",
    "    brightness_range= (0.55, 0.9),\n",
    ")\n",
    "\n",
    "valid_test_datagen = ImageDataGenerator(\n",
    "    rescale= 1./255, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(  \n",
    "    train_data_dir,  \n",
    "    target_size= (img_width, img_height), \n",
    "    color_mode= 'rgb',\n",
    "    batch_size= batch_size,  \n",
    "    class_mode= 'categorical',\n",
    "    shuffle= True, \n",
    "    seed= 1337\n",
    ") \n",
    "\n",
    "valid_generator = valid_test_datagen.flow_from_directory(\n",
    "    val_data_dir,\n",
    "    target_size= (img_width, img_height),\n",
    "    color_mode= 'rgb',\n",
    "    batch_size= batch_size,  \n",
    "    class_mode= 'categorical',\n",
    "    shuffle= True, \n",
    "    seed= 1337\n",
    ")\n",
    "\n",
    "test_generator = valid_test_datagen.flow_from_directory(  \n",
    "    test_data_dir,  \n",
    "    target_size= (img_width, img_height), \n",
    "    color_mode= 'rgb',\n",
    "    batch_size= batch_size,        \n",
    "    class_mode= 'categorical',\n",
    "    shuffle= False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_classes = len(train_generator.class_indices)  \n",
    "train_labels = train_generator.classes \n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "valid_labels = valid_generator.classes \n",
    "valid_labels = to_categorical(valid_labels, num_classes=num_classes)\n",
    "nb_train_samples = len(train_generator.filenames)  \n",
    "nb_valid_samples = len(valid_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Image Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img = load_img(\"C:/Users/Admin/Downloads/ML_lab/Assignment_2_ML/train/NORMAL/NORMAL-9863816-2.jpeg\")\n",
    "data = img_to_array(img)\n",
    "samples = expand_dims(data, 0)\n",
    "it = train_datagen.flow(samples, batch_size=1)\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)\n",
    "\n",
    "for i in range(9):\n",
    "\tplt.subplot(330 + 1 + i)\n",
    "\tbatch = it.next()\n",
    "\timage = batch[0]\n",
    "\tplt.imshow(image)\n",
    "\n",
    "plt.savefig('augmented_image.png', transparent= False, bbox_inches= 'tight', dpi= 400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "* VGG16 CNN architecture is used for calssification.\n",
    "* Pretrained on the 'ImageNet' dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vgg16 = VGG16(include_top= False, input_shape= (img_width, img_height, channels), weights= 'imagenet')\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "for layer in vgg16.layers:\n",
    "    model.add(layer)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable= False\n",
    "\n",
    "model.add(Flatten(input_shape= (4, 4, 512)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4,activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True, expand_nested=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer= keras.optimizers.Adam(lr= 0.0001), loss= 'categorical_crossentropy', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'baseline_model.h5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='auto',\n",
    "    save_weights_only=False,\n",
    "    period=1\n",
    ")\n",
    "\n",
    "earlystop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.001,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "csvlogger = CSVLogger(\n",
    "    filename= \"baseline_training_csv.log\",\n",
    "    separator = \",\",\n",
    "    append = False\n",
    ")\n",
    "\n",
    "reduceLR = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=1, \n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, earlystop, csvlogger,reduceLR]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator, \n",
    "    epochs = 30,\n",
    "    steps_per_epoch = nb_train_samples//batch_size,\n",
    "    validation_data = valid_generator, \n",
    "    validation_steps = nb_valid_samples//batch_size,\n",
    "    verbose = 2,\n",
    "    callbacks = callbacks,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(eval_loss, eval_accuracy) = model.evaluate(test_generator, batch_size= batch_size, verbose= 1)\n",
    "print('Test Loss: ', eval_loss)\n",
    "print('Test Accuracy: ', eval_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplot()\n",
    "plt.rcParams['figure.figsize'] = (6.0, 4.0)\n",
    "plt.title('Baseline Model Accuracy')\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Training Accuracy','Validation Accuracy'])\n",
    "plt.savefig('baseline_acc_epoch.png', transparent= False, bbox_inches= 'tight', dpi= 400)\n",
    "plt.show()\n",
    "\n",
    "plt.subplot()\n",
    "plt.title('Baseline Model Loss')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Training Loss','Validation Loss'])\n",
    "plt.savefig('baseline_loss_epoch.png', transparent= False, bbox_inches= 'tight', dpi= 400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning with Reduced Learning Rate\n",
    "* Unfroze model weights.\n",
    "* Reduced learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.trainable= True\n",
    "model.compile(optimizer= keras.optimizers.Adam(1e-5), loss= 'categorical_crossentropy', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'finetuned_model.h5',\n",
    "    monitor='loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='auto',\n",
    "    save_weights_only=False,\n",
    "    period=1\n",
    ")\n",
    "\n",
    "earlystop = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    min_delta=0.001,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "csvlogger = CSVLogger(\n",
    "    filename= \"finetuned_training_csv.log\",\n",
    "    separator = \",\",\n",
    "    append = False\n",
    ")\n",
    "\n",
    "reduceLR = ReduceLROnPlateau(\n",
    "    monitor='loss',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=1, \n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, earlystop, csvlogger,reduceLR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_1 = model.fit(\n",
    "    train_generator, \n",
    "    epochs = 10,\n",
    "    steps_per_epoch = nb_train_samples//batch_size,\n",
    "    validation_data = valid_generator, \n",
    "    validation_steps = nb_valid_samples//batch_size,\n",
    "    verbose = 2,\n",
    "    callbacks = callbacks,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplot()\n",
    "plt.rcParams['figure.figsize'] = (6.0, 4.0)\n",
    "plt.title('Finetuned Model Accuracy')\n",
    "plt.plot(history_1.history['accuracy'])\n",
    "plt.plot(history_1.history['val_accuracy'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Training Accuracy','Validation Accuracy'])\n",
    "plt.savefig('finetuned_acc_epoch.png', transparent= False, bbox_inches= 'tight', dpi= 400)\n",
    "plt.show()\n",
    "\n",
    "plt.subplot()\n",
    "plt.title('Finetuned Model Loss')\n",
    "plt.plot(history_1.history['loss'])\n",
    "plt.plot(history_1.history['val_loss'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Training Loss','Validation Loss'])\n",
    "plt.savefig('finetuned_loss_epoch.png', transparent= False, bbox_inches= 'tight', dpi= 400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(eval_loss, eval_accuracy) = model.evaluate(test_generator, batch_size= batch_size, verbose= 1)\n",
    "print('Test Loss: ', eval_loss)\n",
    "print('Test Accuracy: ', eval_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(test_generator, nb_test_samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "cm = confusion_matrix(test_generator.classes, y_pred)\n",
    "df_cm = pd.DataFrame(cm, list(test_generator.class_indices.keys()), list(test_generator.class_indices.keys()))\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix\\n')\n",
    "plt.savefig('confusion_matrix.png', transparent= False, bbox_inches= 'tight', dpi= 400)\n",
    "plt.show()\n",
    "\n",
    "print('Classification Report\\n')\n",
    "target_names = list(test_generator.class_indices.keys())\n",
    "print(classification_report(test_generator.classes, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image = load_img('../input/kermany2018/OCT2017 /test/NORMAL/NORMAL-1017237-1.jpeg', target_size= (img_width, img_height))\n",
    "image = img_to_array(image)\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "image = preprocess_input(image)\n",
    "y_hat = model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "activations= keract.get_activations(model, image, layer_names= None, nodes_to_evaluate= None, output_format= 'simple', auto_compile= True)\n",
    "keract.display_activations(activations, save= True, directory= '/kaggle/working/activations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keract.display_heatmaps(activations, image, save= True, directory= '/kaggle/working/heatmaps')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
